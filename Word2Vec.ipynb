{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "unsigned-welcome",
   "metadata": {},
   "source": [
    "# Word2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "genetic-harvest",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "id": "capable-childhood",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "from gensim.models import word2vec\n",
    "import json\n",
    "import spacy\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "phantom-partition",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "id": "detailed-trunk",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(path):\n",
    "    return pd.read_csv(path, header = 0, delimiter = '\\t', quoting = 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "id": "sapphire-swimming",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Data Shape: (25000, 3)\n",
      "Unlabled Train Data Shape: (50000, 2)\n",
      "Test Data Shape: (25000, 2)\n"
     ]
    }
   ],
   "source": [
    "# Train Data\n",
    "train_data = load_data(path = 'Data/Raw/labeledTrainData.tsv')\n",
    "print('Train Data Shape:', train_data.shape)\n",
    "\n",
    "# Unlabeled Train Data\n",
    "unlabled_train_data = load_data(path = 'Data/Raw/unlabeledTrainData.tsv')\n",
    "print('Unlabled Train Data Shape:', unlabled_train_data.shape)\n",
    "\n",
    "# Test Data\n",
    "test_data = load_data(path = 'Data/Raw/testData.tsv')\n",
    "print('Test Data Shape:', test_data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "effective-blues",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "behind-japanese",
   "metadata": {},
   "source": [
    "### Sentence To Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "id": "female-information",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence_to_words(sentence):\n",
    "\n",
    "    # Remove Markups\n",
    "    sentence =  BeautifulSoup(sentence).get_text()\n",
    "\n",
    "    # Remove Numbers\n",
    "    sentence = re.sub('[^a-zA-Z]', ' ', sentence)\n",
    "\n",
    "    # Lemmatize\n",
    "    words = [token.lemma_.lower() for token in nlp(sentence)]\n",
    "\n",
    "    return words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "blocked-sperm",
   "metadata": {},
   "source": [
    "### Review To Sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "id": "manufactured-atmosphere",
   "metadata": {},
   "outputs": [],
   "source": [
    "def review_to_sentences(review):\n",
    "    \n",
    "    sentences = []\n",
    "\n",
    "    # Generate sentences\n",
    "    doc = nlp(review)\n",
    "    review_sentences = [sent.text for sent in doc.sents]\n",
    "    \n",
    "    for sentence in review_sentences:\n",
    "        if len(sentence) > 0:\n",
    "            sentences.append(sentence_to_words(sentence))\n",
    "    \n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "failing-orleans",
   "metadata": {},
   "source": [
    "### List of Sentences of Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "id": "sitting-burden",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing \"Train Data\" 0...\n",
      "Processing \"Unlabeled Train Data\" 0...\n"
     ]
    }
   ],
   "source": [
    "sentences = []\n",
    "\n",
    "# Train Data\n",
    "for i, review in enumerate(train_data['review'][:30]):\n",
    "    sentences += review_to_sentences(review)\n",
    "    \n",
    "    if i % 1000 == 0: print(f'Processing \"Train Data\" {i}...')\n",
    "\n",
    "# Unlabeled Train Data\n",
    "for i, review in enumerate(unlabled_train_data['review'][:30]):\n",
    "    sentences += review_to_sentences(review)\n",
    "    \n",
    "    if i % 1000 == 0: print(f'Processing \"Unlabeled Train Data\" {i}...')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "married-harris",
   "metadata": {},
   "source": [
    "## Save the Sentences into JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "id": "prerequisite-bride",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(r\"Data/Processed/Word2Vec_sentences.json\", \"w\") as file:\n",
    "    json.dump(sentences, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "british-belle",
   "metadata": {},
   "source": [
    "## Load the Sentences from JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "id": "choice-kazakhstan",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = None\n",
    "\n",
    "with open(r\"Data/Processed/Word2Vec_sentences.json\", \"r\") as file:\n",
    "    sentences = json.load(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bibliographic-isaac",
   "metadata": {},
   "source": [
    "## Model (Word2Vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "id": "centered-homework",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_features = 300    # Word vector dimensionality                      \n",
    "min_word_count = 40   # Minimum word count                        \n",
    "num_workers = 4       # Number of threads to run in parallel\n",
    "context = 10          # Context window size                                                                                    \n",
    "downsampling = 1e-3   # Downsample setting for frequent words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "id": "headed-publicity",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = word2vec.Word2Vec(\n",
    "    sentences = sentences,\n",
    "    workers = num_workers,\n",
    "    vector_size = num_features,\n",
    "    min_count = min_word_count,\n",
    "    window = context,\n",
    "    sample = downsampling\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "id": "genuine-representative",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"Data/Processed/300features_40minwords_10context\"\n",
    "model.save(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "victorian-inventory",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cheap-horse",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "contrary-wisconsin",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "perceived-second",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
