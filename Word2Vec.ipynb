{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "unable-bidder",
   "metadata": {},
   "source": [
    "# Word2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pleasant-authentication",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "remarkable-speaker",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "from gensim.models import word2vec\n",
    "import json\n",
    "import multiprocessing\n",
    "from time import time\n",
    "import spacy\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "precious-variation",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "honey-match",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(path):\n",
    "    return pd.read_csv(path, header = 0, delimiter = '\\t', quoting = 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "celtic-blink",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Data Shape: (25000, 3)\n",
      "Unlabled Train Data Shape: (50000, 2)\n",
      "Test Data Shape: (25000, 2)\n"
     ]
    }
   ],
   "source": [
    "# Train Data\n",
    "train_data = load_data(path = 'Data/Raw/labeledTrainData.tsv')\n",
    "print('Train Data Shape:', train_data.shape)\n",
    "\n",
    "# Unlabeled Train Data\n",
    "unlabled_train_data = load_data(path = 'Data/Raw/unlabeledTrainData.tsv')\n",
    "print('Unlabled Train Data Shape:', unlabled_train_data.shape)\n",
    "\n",
    "# Test Data\n",
    "test_data = load_data(path = 'Data/Raw/testData.tsv')\n",
    "print('Test Data Shape:', test_data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "communist-worthy",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "distinct-economy",
   "metadata": {},
   "source": [
    "### Sentence To Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "competent-district",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence_to_words(sentence):\n",
    "\n",
    "    # Remove Markups\n",
    "    sentence =  BeautifulSoup(sentence).get_text()\n",
    "\n",
    "    # Remove Numbers\n",
    "    sentence = re.sub('[^a-zA-Z]', ' ', sentence)\n",
    "    \n",
    "    # Remvoe URLs\n",
    "    sentence = re.sub(r'http\\S+', '', sentence)\n",
    "\n",
    "    # Lemmatize\n",
    "    words = [token.lemma_.lower() for token in nlp(sentence)]\n",
    "\n",
    "    return words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "spread-telling",
   "metadata": {},
   "source": [
    "### Review To Sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "vocational-sharp",
   "metadata": {},
   "outputs": [],
   "source": [
    "def review_to_sentences(review):\n",
    "    \n",
    "    sentences = []\n",
    "\n",
    "    # Generate sentences\n",
    "    doc = nlp(review)\n",
    "    review_sentences = [sent.text for sent in doc.sents]\n",
    "    \n",
    "    for sentence in review_sentences:\n",
    "        if len(sentence) > 0:\n",
    "            sentences.append(sentence_to_words(sentence))\n",
    "    \n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "direct-reputation",
   "metadata": {},
   "source": [
    "### List of Sentences of Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "shaped-company",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing \"Train Data\" 0...\n",
      "Processing \"Train Data\" 100...\n",
      "Processing \"Train Data\" 200...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\installation root\\python 3.8.7\\lib\\site-packages\\bs4\\__init__.py:332: MarkupResemblesLocatorWarning: \"...\" looks like a filename, not markup. You should probably open this file and pass the filehandle into Beautiful Soup.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing \"Train Data\" 300...\n",
      "Processing \"Train Data\" 400...\n",
      "Processing \"Unlabeled Train Data\" 0...\n",
      "Processing \"Unlabeled Train Data\" 100...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\installation root\\python 3.8.7\\lib\\site-packages\\bs4\\__init__.py:332: MarkupResemblesLocatorWarning: \".\" looks like a filename, not markup. You should probably open this file and pass the filehandle into Beautiful Soup.\n",
      "  warnings.warn(\n",
      "e:\\installation root\\python 3.8.7\\lib\\site-packages\\bs4\\__init__.py:332: MarkupResemblesLocatorWarning: \"..........\" looks like a filename, not markup. You should probably open this file and pass the filehandle into Beautiful Soup.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing \"Unlabeled Train Data\" 200...\n",
      "Processing \"Unlabeled Train Data\" 300...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\installation root\\python 3.8.7\\lib\\site-packages\\bs4\\__init__.py:417: MarkupResemblesLocatorWarning: \"http://www.archive.org/details/LovefromaStranger\"\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing \"Unlabeled Train Data\" 400...\n"
     ]
    }
   ],
   "source": [
    "sentences = []\n",
    "\n",
    "# Train Data\n",
    "for i, review in enumerate(train_data['review'][:500]):\n",
    "    sentences += review_to_sentences(review)\n",
    "    \n",
    "    if i % 100 == 0: print(f'Processing \"Train Data\" {i}...')\n",
    "\n",
    "# Unlabeled Train Data\n",
    "for i, review in enumerate(unlabled_train_data['review'][:500]):\n",
    "    sentences += review_to_sentences(review)\n",
    "    \n",
    "    if i % 100 == 0: print(f'Processing \"Unlabeled Train Data\" {i}...')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "graphic-atlanta",
   "metadata": {},
   "source": [
    "## Save the Sentences into JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "employed-container",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(r\"Data/Processed/Word2Vec_sentences.json\", \"w\") as file:\n",
    "    json.dump(sentences, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sudden-trout",
   "metadata": {},
   "source": [
    "## Load the Sentences from JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "changing-philip",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = None\n",
    "\n",
    "with open(r\"Data/Processed/Word2Vec_sentences.json\", \"r\") as file:\n",
    "    sentences = json.load(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "referenced-effort",
   "metadata": {},
   "source": [
    "## Model (Word2Vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "spread-column",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_features = 300    # Word vector dimensionality                      \n",
    "min_word_count = 40   # Minimum word count                        \n",
    "num_workers = 4       # Number of threads to run in parallel\n",
    "context = 10          # Context window size                                                                                    \n",
    "downsampling = 1e-3   # Downsample setting for frequent words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "executed-combine",
   "metadata": {},
   "source": [
    "### Build the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "affected-means",
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_model = word2vec.Word2Vec(\n",
    "    workers = num_workers,\n",
    "    vector_size = num_features,\n",
    "    min_count = min_word_count,\n",
    "    window = context,\n",
    "    sample = downsampling\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "comic-station",
   "metadata": {},
   "source": [
    "### Initialize the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "limiting-december",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to build vocab: 0.0 mins\n"
     ]
    }
   ],
   "source": [
    "t = time()\n",
    "\n",
    "w2v_model.build_vocab(sentences, progress_per = 10000)\n",
    "\n",
    "print('Time to build vocab: {} mins'.format(round((time() - t) / 60, 2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "appointed-syntax",
   "metadata": {},
   "source": [
    "### Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "consolidated-ireland",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to train the model: 0.05 mins\n"
     ]
    }
   ],
   "source": [
    "t = time()\n",
    "\n",
    "w2v_model.train(sentences, total_examples = w2v_model.corpus_count, epochs = 30, report_delay = 1)\n",
    "\n",
    "print('Time to train the model: {} mins'.format(round((time() - t) / 60, 2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fiscal-turkey",
   "metadata": {},
   "source": [
    "## Save the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "signal-group",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"Data/Processed/word2vec_model\"\n",
    "model.save(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "separated-burner",
   "metadata": {},
   "source": [
    "## Evaluate the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "final-sheet",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('sequence', 0.6082822680473328),\n",
       " ('shot', 0.5585779547691345),\n",
       " ('moment', 0.5281933546066284),\n",
       " ('side', 0.41745156049728394),\n",
       " ('hilarious', 0.41189679503440857),\n",
       " ('episode', 0.39389824867248535),\n",
       " ('violence', 0.37117457389831543),\n",
       " ('car', 0.36822807788848877),\n",
       " ('cut', 0.35873159766197205),\n",
       " ('score', 0.3525751531124115)]"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v_model.wv.most_similar(\"scene\".split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "loaded-adobe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.57044554"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v_model.wv.similarity(\"death\", \"war\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "cloudy-conservative",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('sequence', 0.6235834360122681),\n",
       " ('violence', 0.5777526497840881),\n",
       " ('score', 0.543147087097168),\n",
       " ('shot', 0.5370824337005615),\n",
       " ('musical', 0.4834674298763275),\n",
       " ('event', 0.47258278727531433),\n",
       " ('dialogue', 0.4615626931190491),\n",
       " ('title', 0.4596373438835144),\n",
       " ('side', 0.44574621319770813),\n",
       " ('country', 0.4351249933242798)]"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v_model.wv.most_similar(\"scene war music\".split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "above-ticket",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'music'"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v_model.wv.doesnt_match(['death', 'war', 'music'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adequate-practice",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "frank-period",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "unlimited-concentrate",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "correct-operations",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
