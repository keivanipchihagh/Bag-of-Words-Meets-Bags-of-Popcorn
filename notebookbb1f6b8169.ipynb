{"cells":[{"cell_type":"markdown","metadata":{"id":"d6HdrSKd73TN"},"source":["# Bag of Words Meets Bags of Popcorn"]},{"cell_type":"code","execution_count":2,"metadata":{"executionInfo":{"elapsed":4104,"status":"ok","timestamp":1628412385633,"user":{"displayName":"Keivan Ipchi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiYWJZ-pareZcENvSntLEr0hS7wN6xv1TYBeXeu=s64","userId":"05399847325073645104"},"user_tz":-270},"id":"Jd0NbFKT73TQ"},"outputs":[],"source":["# General Python packages\n","import re\n","import spacy\n","import string\n","import pandas as pd\n","from collections import Counter\n","\n","# Keras packages\n","from keras.preprocessing.text import Tokenizer\n","from keras.preprocessing.sequence import pad_sequences\n","from keras.utils.vis_utils import plot_model\n","from keras.models import Sequential\n","from keras.layers import Dense, Embedding\n","\n","nlp = spacy.load('en_core_web_sm')   # Also 'en_core_web_md' and 'en_core_web_lg'."]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":203},"executionInfo":{"elapsed":3371,"status":"ok","timestamp":1628412389002,"user":{"displayName":"Keivan Ipchi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiYWJZ-pareZcENvSntLEr0hS7wN6xv1TYBeXeu=s64","userId":"05399847325073645104"},"user_tz":-270},"id":"Z2nhTTZ073TS","outputId":"e2ce19cf-f385-48a8-bfe1-7cdba103e952"},"outputs":[{"data":{"text/html":["\u003cdiv\u003e\n","\u003cstyle scoped\u003e\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","\u003c/style\u003e\n","\u003ctable border=\"1\" class=\"dataframe\"\u003e\n","  \u003cthead\u003e\n","    \u003ctr style=\"text-align: right;\"\u003e\n","      \u003cth\u003e\u003c/th\u003e\n","      \u003cth\u003eid\u003c/th\u003e\n","      \u003cth\u003esentiment\u003c/th\u003e\n","      \u003cth\u003ereview\u003c/th\u003e\n","    \u003c/tr\u003e\n","  \u003c/thead\u003e\n","  \u003ctbody\u003e\n","    \u003ctr\u003e\n","      \u003cth\u003e0\u003c/th\u003e\n","      \u003ctd\u003e5814_8\u003c/td\u003e\n","      \u003ctd\u003e1\u003c/td\u003e\n","      \u003ctd\u003eWith all this stuff going down at the moment w...\u003c/td\u003e\n","    \u003c/tr\u003e\n","    \u003ctr\u003e\n","      \u003cth\u003e1\u003c/th\u003e\n","      \u003ctd\u003e2381_9\u003c/td\u003e\n","      \u003ctd\u003e1\u003c/td\u003e\n","      \u003ctd\u003e\\The Classic War of the Worlds\\\" by Timothy Hi...\u003c/td\u003e\n","    \u003c/tr\u003e\n","    \u003ctr\u003e\n","      \u003cth\u003e2\u003c/th\u003e\n","      \u003ctd\u003e7759_3\u003c/td\u003e\n","      \u003ctd\u003e0\u003c/td\u003e\n","      \u003ctd\u003eThe film starts with a manager (Nicholas Bell)...\u003c/td\u003e\n","    \u003c/tr\u003e\n","    \u003ctr\u003e\n","      \u003cth\u003e3\u003c/th\u003e\n","      \u003ctd\u003e3630_4\u003c/td\u003e\n","      \u003ctd\u003e0\u003c/td\u003e\n","      \u003ctd\u003eIt must be assumed that those who praised this...\u003c/td\u003e\n","    \u003c/tr\u003e\n","    \u003ctr\u003e\n","      \u003cth\u003e4\u003c/th\u003e\n","      \u003ctd\u003e9495_8\u003c/td\u003e\n","      \u003ctd\u003e1\u003c/td\u003e\n","      \u003ctd\u003eSuperbly trashy and wondrously unpretentious 8...\u003c/td\u003e\n","    \u003c/tr\u003e\n","  \u003c/tbody\u003e\n","\u003c/table\u003e\n","\u003c/div\u003e"],"text/plain":["       id  sentiment                                             review\n","0  5814_8          1  With all this stuff going down at the moment w...\n","1  2381_9          1  \\The Classic War of the Worlds\\\" by Timothy Hi...\n","2  7759_3          0  The film starts with a manager (Nicholas Bell)...\n","3  3630_4          0  It must be assumed that those who praised this...\n","4  9495_8          1  Superbly trashy and wondrously unpretentious 8..."]},"execution_count":3,"metadata":{"tags":[]},"output_type":"execute_result"}],"source":["train_df = pd.read_csv('/content/drive/MyDrive/Kaggle/Bag-of-Words-Meets-Bags-of-Popcorn/Data/Raw/labeledTrainData.tsv', delimiter = \"\\t\")\n","test_df = pd.read_csv('/content/drive/MyDrive/Kaggle/Bag-of-Words-Meets-Bags-of-Popcorn/Data/Raw/testData.tsv', delimiter = \"\\t\")\n","combined = [train_df, test_df]\n","\n","train_df.head()"]},{"cell_type":"markdown","metadata":{"id":"j20rifn573TV"},"source":["# Text Preprocessing"]},{"cell_type":"code","execution_count":4,"metadata":{"executionInfo":{"elapsed":4,"status":"ok","timestamp":1628412389003,"user":{"displayName":"Keivan Ipchi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiYWJZ-pareZcENvSntLEr0hS7wN6xv1TYBeXeu=s64","userId":"05399847325073645104"},"user_tz":-270},"id":"LgrXb6pM73TV"},"outputs":[],"source":["def remove_punctuations(text):\n","    return text.replae(string.punctuation, ' ')\n","\n","def clean(review):    \n","    \n","    # Early manual fix\n","    review = review.replace(\"\\\\\", '')\n","    \n","    # Combine Filters for HTML, Paranteshis, Non-ASCII and Numbers\n","    filters = [r'\u003c\\w+ ?/\u003e', r'\\([^()]*\\)', r'[^\\x00-\\x7F]+', r'(\\d+)']\n","    review = re.sub(r'|'.join(filters), ' ', review)\n","        \n","    # Convert to lowercase\n","    review = review.lower()\n","\n","    # Lemmatize and remove punctuations\n","    review = ' '.join([token.lemma_ for token in nlp(review) if (not token.is_punct) and (not token.is_stop)])    \n","    \n","    # Late Manual fix\n","    review = review.replace(\"\\'s\", '')\n","    review = review.replace(\"-PRON-\", '')\n","    \n","    # Remove remaining punctuations\n","    review = review.replace(string.punctuation, ' ')\n","    \n","    # Filter-out words with less than 3 character\n","    review = ' '.join([word for word in review.split() if len(word) \u003e 2])\n","    \n","    return review"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/"},"id":"FY3W2j7P73TW"},"outputs":[{"name":"stdout","output_type":"stream","text":["1th dataset cleaned.\n","2th dataset cleaned.\n"]}],"source":["for i, df in enumerate(combined):\n","    df['cleaned_review'] = df['review'].apply(clean)\n","    print(f'{i + 1}th dataset cleaned.')"]},{"cell_type":"markdown","metadata":{"id":"aZTj_P9P73TX"},"source":["# Building the vocabulary"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"8fekZ5qo73T2"},"outputs":[{"name":"stdout","output_type":"stream","text":["Top 5 most common words: [('movie', 50588), ('film', 46543), ('like', 21324), ('good', 20087), ('time', 15334)]\n","Vocabulary size: 65821\n"]}],"source":["vocab = Counter()\n","\n","for i, review in train_df.iterrows():\n","    vocab.update(review['cleaned_review'].split())\n","\n","# Print most common words\n","print('Top 5 most common words:', vocab.most_common(5))\n","\n","vocab_size = len(vocab)\n","print('Vocabulary size:', vocab_size)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1lIhAxbV73T7"},"outputs":[],"source":["# Tokenize reviews\n","def tokenize(sentences, vocab_size, mode):\n","    tokenizer = Tokenizer(num_words = vocab_size, oov_token = '?')\n","    tokenizer.fit_on_texts(sentences)\n","    return tokenizer.texts_to_matrix(sentences, mode = mode)\n","\n","# Modes: 'binary', 'count', 'freq', 'tfidf'\n","X_train = tokenize(train_df['cleaned_review'], vocab_size, 'binary')\n","X_test = tokenize(test_df['cleaned_review'], vocab_size, 'binary')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lZ0SCalE73T_"},"outputs":[],"source":["# Padding the encodings\n","def pad(encoding, maxlen, padding = 'pre', truncating = 'post'):    \n","    return pad_sequences (\n","        sequences = encoding,\n","        maxlen = maxlen,\n","        padding = padding,\n","        truncating = truncating,\n","        value = 0\n","    )\n","\n","maxlen = 500\n","X_train = pad(encoding = X_train)\n","X_test = pad(encoding = X_test)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7zP_kJZV73UA"},"outputs":[],"source":["# Model structure\n","model = Sequential()\n","model.add(Dense(units = 64, activation = 'relu', inpu_dim = (X_train.shape[0], )))\n","model.add(Dense(units = 32, activation = 'relu'))\n","model.add(Dense(units = 1, activation = 'sigmoid'))\n","model.compile(loss = 'binary_crossentropy', optimizer = 'adam', metrics = ['accuracy'])\n","plot_model(model = model, show_shapes = True)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Kh-HErU073UB"},"outputs":[],"source":["# Train the Model\n","history = model.fit(\n","    x = ,\n","    y = ,\n","    epochs = 10,\n","    batch_size = 64,\n","    validation_split = 0.2\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Jp4Kqsla73UC"},"outputs":[],"source":["# Plot the Model\n","epochs = [i for i in range(len(history.history['loss']))]\n","\n","def plot_subplot(axs, metric, val_metric):\n","    ''' Plot a single subplot '''\n","\n","    axs.set_title('Analysis of ' + metric)\n","    axs.plot(epochs, history.history[metric], label = metric)\n","    axs.plot(epochs, history.history[val_metric], label = val_metric)\n","    axs.legend()\n","\n","fig, axs = plt.subplots(1, len(metrics), figsize = (18, 5))\n","\n","for i, metric in enumerate(['accuracy', 'loss']):\n","    plot_subplot(axs[i], metric, 'val_' + metric)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"oWijCr8973UC"},"outputs":[],"source":["# Predict and Submission\n","y_pred = model.predict(X_test)"]}],"metadata":{"colab":{"collapsed_sections":[],"name":"notebookbb1f6b8169.ipynb","toc_visible":true,"version":""},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.10"}},"nbformat":4,"nbformat_minor":0}